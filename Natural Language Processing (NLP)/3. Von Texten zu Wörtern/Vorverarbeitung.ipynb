{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "30071801-4b6b-4d28-9826-a627087b160a"
    }
   },
   "source": [
    "# Vorverarbeitung von Texten mit Python und NLTK\n",
    "\n",
    "\n",
    "Für viele Aufgaben müssen Texte immer auf der gleichen Art analysiert werden. Für eine Aufgabe wie die Sprachidentifikation können wir einen Text als eine lange Zeichenkette betrachten. Meistens brauchen wir aber eine Liste von Wörtern, mit denen wir weiter arbeiten können. \n",
    "\n",
    "Wenn wir erstmal den Text haben (was oft nicht einfach ist, wenn der Text z.B. aus einer Webseite extrahiert werden muss), teilen wir den Text zuerst in Sätze und die Sätze anschließend in Wörtern auf. In vielen Fällen führen wir die Wörter dann noch auf ihren Grundform zurück, und bestimmen die Wortart für jedes Wort, da diese oft wichtige Informationen für die wietere Verarbeitung gibt. Eventuell folgen dann Schritte, wie das Nachschlagen der Wörter in einem Thesaurus oder das Erkennen von sogenannten _Named Entities_: Namen von Personen, Institutionen, Produkte, usw.\n",
    "\n",
    "![Typische Verarbeitungsschritte bei der Textanalyse](http://textmining.wp.hs-hannover.de/img/pipeline.jpg)\n",
    "\n",
    "Für die Analyse von Englischen Texten sind weitaus mehr Werkzeuge verfügbar als für das Deutsche. Die Verarbeitung von englischen Texten ist daher etwas einfacher. Das wir außerdem auch of englische Texte analysieren müssen, schauen wir uns die Standardverarbeitung zunächst für das Englische an. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "28f5a8ff-008f-4595-a04b-f7fd4fc27c83"
    }
   },
   "source": [
    "## Englische Texte\n",
    "\n",
    "Wenn wir mit Python Texte analysieren wollen, ist das Paket NLTK der Stanford University unverzichtbar. Dieses Paket umfasst State-of-the-Art Implementierungen für so gut wie alle wichtige Algorithmen aus der Sprachverarbeitung und ist in den meisten Python-Distributionen enthalten.\n",
    "\n",
    "NLTK enthält auch eine große Menge Ressourcen, die Sie nutzen können, oder die manche der bereitgestellten Algorithmen brauchen. Diese sind meistens noch nicht installiert. Das nachinstallieren dieser Ressourcen ist ganz einfach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk #natural language toolkit\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es sollte jetz ein Fernster geöffnet werden. Wählen Sie alles aus dem NLTK-Buch zum installieren aus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "a0a1ea8c-f8b7-4969-9623-c7e338bdc707"
    }
   },
   "source": [
    "### Satzerkennung und Tokenization \n",
    "\n",
    "Mit der Python-Funktion _Split()_ können wir einen Text leicht aufteilen. In vielen Fällen machen wir dann aber Fehler. Es fängt schon damit an, dass Satzzeichen am vorangehenden Wort geschrieben werden, aber nicht dazu gehören, es sei denn, das Wort ist eine Abkürzung oder eine Ordinalzahl, aber letzteres nur im Deutschen. Statt uns über alle Ausnahmen Gedanken zu machen, nutzen wir hierfür einfach eine Funktion aus NLTK. Diese Funktion ist übrigens nicht auf Regeln basiert, sondern wurde aus vielen Beispielen gelernt. \n",
    "\n",
    "Im nächsten Beispiel sehen wir, wie man mit Python und NLTK eine Zeichenkette in eine Liste von Wörtern aufteilen kann. Wir finden nicht nur Wörter, sondern auch Satzzeichen, Zahlen und Symbole. Der Sammelbegriff für diese Einheiten ist _Token_. Das Zerlegen einer Zeichenkette in Tokens wird daher Tokenisierung oder auf Englisch _Tokenization_ genannt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-11T23:41:31.475200Z",
     "start_time": "2017-10-11T23:41:31.467200Z"
    },
    "nbpresent": {
     "id": "d81160be-4c5c-4508-8438-fbaaa11474be"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "sentence= \"At eight o'clock on Thursday morning... Arthur didn't feel very good.\"\n",
    "tokens = nltk.word_tokenize(sentence, language='english') #'german'\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In vielen Fällen ist es wichtig zu wissen, wo die Satzgrenzen sind: Die Wörter in einem Satz haben eine viel engere Beziehung zueinander, als Wörter in verscheidenen Sätzen. Zum Beispiel stehen Begriffe, die aus mehreren Wörtern bestehen, wie \"Bundesministerium für Gesundheit\" immer innerhalb eines Satzes.\n",
    "\n",
    "Die Frage ist nun, ob wir den Text erst in Wörter aufteilen und dann in Sätzen oder umgekehrt. Um zu entscheiden, ob ein Punkt ein Satzende markiert, muss man unter anderem Wissen, ob das Wort vor dem Punkt eine Abkürzung ist. Man muss das Wort also schon mal in der langen Zeichenkette erkannt haben. Der _Sentence Splitter_ von NLTK arbeitet aber trotzdem auf ganzen Texten und braucht keine vorangehende Zerlegung in Tokens. \n",
    "\n",
    "Zum Ausprobieren, lesen wir einen kurzen englischen Text ein. Den hier genutzten Beispieltext finden Sie hier: http://textmining.wp.hs-hannover.de/texte/hanover.txt . Der Text ist der Wikipediaseite http://en.wikipedia.org/wiki/Hanover.html entnommen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l=[1,2,3,4,5,6]\n",
    "# for e in l:\n",
    "#     if e%2==0:\n",
    "#         print(e)\n",
    "gerade=[e for e in l if e==3]\n",
    "print(gerade)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-11T23:41:33.353200Z",
     "start_time": "2017-10-11T23:41:33.325200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Hanover', 'or', 'Hannover', '(', '/ˈhænoʊvər/', ';', 'German', ':', 'Hannover', ',', 'pronounced', '[', 'haˈnoːfɐ', ']', '(', 'About', 'this', 'sound', 'listen', ')', ')', ',', 'on', 'the', 'River', 'Leine', ',', 'is', 'the', 'capital', 'and', 'largest', 'city', 'of', 'the', 'German', 'state', 'of', 'Lower', 'Saxony', '(', 'Niedersachsen', ')', ',', 'and', 'was', 'once', 'by', 'personal', 'union', 'the', 'family', 'seat', 'of', 'the', 'Hanoverian', 'Kings', 'of', 'the', 'United', 'Kingdom', 'of', 'Great', 'Britain', 'and', 'Ireland', ',', 'under', 'their', 'title', 'as', 'the', 'dukes', 'of', 'Brunswick-Lüneburg', '(', 'later', 'described', 'as', 'the', 'Elector', 'of', 'Hanover', ')', '.'], ['At', 'the', 'end', 'of', 'the', 'Napoleonic', 'Wars', ',', 'the', 'Electorate', 'was', 'enlarged', 'to', 'become', 'a', 'Kingdom', 'with', 'Hanover', 'as', 'its', 'capital', '.'], ['From', '1868', 'to', '1946', 'Hanover', 'was', 'the', 'capital', 'of', 'the', 'Prussian', 'Province', 'of', 'Hanover', 'and', 'afterwards', 'of', 'the', 'Hanover', 'administrative', 'region', 'until', 'that', 'was', 'abolished', 'in', '2005', '.']]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import codecs\n",
    "\n",
    "textfile = codecs.open(\"texte/hanover.txt\", \"r\", \"utf-8-sig\")\n",
    "text = textfile.read()\n",
    "textfile.close()\n",
    "\n",
    "sentences = nltk.sent_tokenize(text,language='english') #Liste Sätze\n",
    "\n",
    "#print(sentences)\n",
    "\n",
    "tokenized_text = [nltk.word_tokenize(sent, language='english') for sent in sentences]\n",
    "#tokenized_text=[]\n",
    "#for sent in sentences:\n",
    "#    words=nltk.word_tokenize(sent,language='english')\n",
    " #   tokenized_text.append(words)\n",
    "\n",
    "\n",
    "print(tokenized_text[0:3])\n",
    "#print(tokenized_text[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wortarterkennung (Part-of-Speech Tagging)\n",
    "\n",
    "Die Wortart eines Wortes gibt oft wichtige Informationen. Den Hauptinhalt eines Textes erkennen wir beispielsweise schon an den benutzten Substantiven und Verben, während Adjektive und Adverbien wenig beitragen, und Artikel, Präpositionen und Hilfsverben hierzu überhaupt keine nützliche Information liefern. \n",
    "\n",
    "Wortarten werden im Englischen _Part of Speech_ genannt, ein Programm, dass die Wortarten zuweist, daher _Part of speaach tagger_ oder einfach _POS tagger_. Der NLTK enthält einen guten (statistischen) POS Tagger.\n",
    "\n",
    "Die Wortklassen, die dieser Tagger zuweist, sind nicht genau die, die Sie in der Schule gelernt haben. Manche Klassen sind unterteilt, es gibt Tags, die neben der Wortklasse weitere Informationen, wie z.B. 3. Person Singular enthalten und es gibt Klassen für Wörter, die oft schwierig einzuteilen sind. Die Tags, die im folgenden Beispiel genutzt werden, sind die aus dem sogenannten Pennsylvenia Treebank Tagset. Eine Beschreibung der Tags sowie Beispiel dafür bekommen Sie mit der help Funktion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-11T23:41:35.272200Z",
     "start_time": "2017-10-11T23:41:35.246200Z"
    }
   },
   "outputs": [],
   "source": [
    "nltk.help.upenn_tagset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-11T12:48:20.712587Z",
     "start_time": "2017-10-11T12:48:20.703587Z"
    }
   },
   "source": [
    "Jetzt aber ein Beispiel für den Tagger:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-11T23:41:37.305200Z",
     "start_time": "2017-10-11T23:41:37.182200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "haˈnoːfɐ\n",
      "sound\n",
      "capital\n",
      "city\n",
      "state\n",
      "union\n",
      "family\n",
      "seat\n",
      "title\n"
     ]
    }
   ],
   "source": [
    "tags = nltk.pos_tag(tokenized_text[0])\n",
    "\n",
    "for (word,tag) in tags:\n",
    "    if tag=='NN':\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatisierung\n",
    "\n",
    "In vielen Sprachen, wie auch im Deutschen und Englischen, können Wörter in verschiedenen Formen auftreten. (Es gibt auch Sprachen, in denen das nicht der Fall ist. Diese Sprachen werden isolierende Sprachen genannt. Beispiele hierfür sind Mandarin (Chinesisch) und Vietnamesisch)). Oft ist es wichtig, den Grundfom eines Wortes, das im Text in flektierter Form vorkommt, zu bestimmen. Es ist wichtig, dass wir hier drei Begriffe klar trennen:\n",
    "* Lemma - Die Form des Wortes, wie sie in einem Wörterbuch steht. Z.B.: Haus, laufen, begründen\n",
    "* Stamm - Das Wort ohne Flexionsendungen (Prefixe und Suffixe). Z.B.: Haus, lauf, begründ\n",
    "* Wurzel - Kern des Wortes, von dem das Wort ggf. durch Derivation abgeleitet wurde. Z.B.: Haus, lauf, Grund\n",
    "\n",
    "Wir unterscheiden jetzt _Stemmer_, Programme, die den Stamm eines Wortes suchen, und _Lemmatisierer_, die das Lemma für jedes Wort suchen. Manche Stemmer trennen auch produktive Derivationssuffixe ab, und geben in vielen Fällen nicht den Stamm, sondern den Wurzel eines Wortes. Es wird oft davon ausgegangen, dass dies für Information Retrieval von Vorteil ist. Wenn man beispielsweise nach _analysieren_ sucht, möchte man wahrscheinlich nicht nur Ergebnisse mit _analysiere_, _analysiert_ , _analysierende_, usw. haben, sondern vermutlich auch welche, in denen nur das Wort _Analyse_ vorkommt. Man kann aber genau so Negativbeispiele finden. Ob diese Art von Stemming wirklich nützlich ist für Information Retrieval, ist nicht eindeutig gekärt (vgl. z.B.:  _BRANTS, Thorsten. Natural Language Processing in Information Retrieval. In: CLIN. 2003._).\n",
    "\n",
    "Ein guter Lemmatizer, der im NLTK enthalten ist, ist der WordNet-Stemmer, der die Vollformen einfach im online-Wörterbuch WordNet nachschlägt. Da ein Wort im Englischen oft zu mehreren Klassen gehören kann, braucht der Wordnet-Lemmatizer auch die Wortklasse. Wir brauchen jetzt ein paar Zeilen Code, um die Penn Treebank Tags in Wordnet-Klassen zu übersetzen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-12T00:07:05.449200Z",
     "start_time": "2017-10-12T00:07:05.422200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hanover', 'or', 'Hannover', '(', '/ˈhænoʊvər/', ';', 'German', ':', 'Hannover', ',', 'pronounce', '[', 'haˈnoːfɐ', ']', '(', 'About', 'this', 'sound', 'listen', ')', ')', ',', 'on', 'the', 'River', 'Leine', ',', 'be', 'the', 'capital', 'and', 'large', 'city', 'of', 'the', 'German', 'state', 'of', 'Lower', 'Saxony', '(', 'Niedersachsen', ')', ',', 'and', 'be', 'once', 'by', 'personal', 'union', 'the', 'family', 'seat', 'of', 'the', 'Hanoverian', 'Kings', 'of', 'the', 'United', 'Kingdom', 'of', 'Great', 'Britain', 'and', 'Ireland', ',', 'under', 'their', 'title', 'as', 'the', 'duke', 'of', 'Brunswick-Lüneburg', '(', 'later', 'describe', 'as', 'the', 'Elector', 'of', 'Hanover', ')', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "#treetagger\n",
    "def wntag(pttag):\n",
    "    if pttag in ['JJ', 'JJR', 'JJS']:\n",
    "        return wn.ADJ\n",
    "    elif pttag in ['NN', 'NNS', 'NNP', 'NNPS']:\n",
    "        return wn.NOUN\n",
    "    elif pttag in ['RB', 'RBR', 'RBS']:\n",
    "        return wn.ADV\n",
    "    elif pttag in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']:\n",
    "        return wn.VERB\n",
    "    return None\n",
    "\n",
    "def lemmatize(lemmatizer,word,pos):\n",
    "    if pos == None:\n",
    "        return word\n",
    "    else:\n",
    "        return lemmatizer.lemmatize(word,pos)\n",
    "\n",
    "lemmata = [lemmatize(lemmatizer,word,wntag(pos)) for (word,pos) in tags]\n",
    "print(lemmata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Etwas einfacher ist der sogenannt _Porter Stemmer_. Der Porterstemmer benutzt kein Wörterbuch sondern hat nur eine Liste von Suffixen, die abgetrennt oder ersetzt werden. Dies führt in vielen Fällen zu unsinnigen Ergebnisse. Oft ist das aber unproblematisch, so lange verschiedene Formen eines Wortes auf dem gleichen eindeutigen Stamm zurückgeführt werden. Neben dem Porter Stemmer enthält NLTK den Lancaster Stemmer, der nach dem gleichen Prinzip arbeitet. Schauen Sie sich die Ergebnisse genau an und vergleichen die Sie die STämme mit den Lemmata des Wordnet-Stemmers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-12T00:14:12.233200Z",
     "start_time": "2017-10-12T00:14:12.221200Z"
    }
   },
   "outputs": [],
   "source": [
    "porter = nltk.PorterStemmer()\n",
    "lancaster = nltk.LancasterStemmer()\n",
    "\n",
    "print(\"Porter Stemmer:\")\n",
    "stems = [porter.stem(t) for t in tokenized_text[0]]\n",
    "print(stems)\n",
    "\n",
    "print(\"\\nLancaster Stemmer:\")\n",
    "stems = [lancaster.stem(t) for t in tokenized_text[0]]\n",
    "print(stems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "1f718d0b-7381-44ef-89e1-3f301ac85d38"
    }
   },
   "source": [
    "## Deutsch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "2679e2d6-048a-408d-89c3-b20f85274a9b"
    }
   },
   "source": [
    "Deutsch und Englsich sind Sprachen, die sich in vielen Hinsichten zielich ähnlich sind. Die Verarbeitungsschritte für einen Deutschen Text unerscheiden sich daher nicht wesentlich von denen für englische Texte. Bei der Tokenisierung und Satzerkennung müssen wir lediglich Deutsch als Parameter angeben, damit besonderheiten des Deutschen besser berücksichtigt werden.\n",
    "\n",
    "Zum Ausprobieren, lesen wir wieder  einen kurzen Text ein. Den hier genutzten Beispieltext finden Sie hier: http://textmining.wp.hs-hannover.de/texte/syrien.txt . Der Text ist der Wikipediaseite http://de.wikipedia.org/wiki/Syrien.html entnommen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-13T15:09:11.664000Z",
     "start_time": "2017-10-13T15:09:11.601000Z"
    },
    "nbpresent": {
     "id": "0055c2a0-0037-4240-89bf-34c724d5d182"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Sie', 'leben', 'zu', 'etwa', 'Dreiviertel', 'in', 'Aleppo', 'und', 'zu', 'knapp', '20', 'Prozent', 'in', 'Damaskus', '.'], ['Die', 'Übrigen', 'verteilen', 'sich', 'auf', 'die', 'größeren', 'Städte', ',', 'besonders', 'in', 'der', 'Dschazira-Region', '.'], ['Armenier', 'gehören', 'überwiegend', 'der', 'Armenischen', 'Apostolischen', 'Kirche', 'an', ',', 'andere', 'sind', 'armenisch-katholisch', '.'], ['Die', 'meisten', 'sind', 'in', 'Handel', ',', 'Kleinindustrie', 'und', 'Handwerk', 'wirtschaftlich', 'erfolgreich', '.'], ['Die', 'meist', 'sunnitischen', 'Turkmenen', 'waren', 'traditionell', 'halbnomadische', 'Viehzüchter', 'in', 'der', 'Dschazira', 'und', 'am', 'unteren', 'Euphrat', 'sowie', 'Ackerbauern', 'um', 'Aleppo', '.'], ['Sie', 'haben', 'sich', 'weitgehend', 'in', 'der', 'arabischen', 'Gesellschaft', 'assimiliert', '.'], ['Tscherkessen', ',', 'ebenfalls', 'Sunniten', ',', 'wurden', 'Ende', 'des', '19.', 'Jahrhunderts', 'aus', 'dem', 'Kaukasus', 'vertrieben', 'und', 'siedelten', 'sich', 'in', 'der', 'Hauran-Region', ',', 'besonders', 'um', 'Quneitra', 'an', ',', 'wo', 'sie', 'sich', 'auf', 'den', 'Anbau', 'von', 'Getreide', 'und', 'daneben', 'Viehzucht', 'spezialisiert', 'haben', '.'], ['Für', 'das', 'Jahr', '1979', 'wurde', 'ihre', 'Zahl', 'auf', '55.000', 'geschätzt', '.'], ['Da', 'viele', 'von', 'ihnen', 'während', 'der', 'französischen', 'Kolonialzeit', 'in', 'der', 'französischen', 'Armee', 'gedient', 'hatten', ',', 'wurden', 'sie', 'lange', 'Zeit', 'von', 'den', 'Arabern', 'argwöhnisch', 'beobachtet', '.'], ['[', '17', ']', 'Die', 'Aramäer', 'und', 'Assyrer', 'gehören', 'einer', 'der', 'christlichen', 'Religionsgemeinschaften', 'an', ',', 'mehrheitlich', 'der', 'Syrisch-Orthodoxen', 'Kirche', 'von', 'Antiochien', '.']]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import codecs\n",
    "\n",
    "textfile = codecs.open(\"texte/syrien.txt\", \"r\", \"utf-8\")\n",
    "text = textfile.read()\n",
    "textfile.close()\n",
    "\n",
    "sentences = nltk.sent_tokenize(text,language='german')\n",
    "#print(sentences[20:23])\n",
    "\n",
    "tokenized_sent = nltk.word_tokenize(sentences[100],language='german')\n",
    "\n",
    "tokenized_text=[nltk.tokenize.word_tokenize(sent,language='german') for sent in sentences]\n",
    "\n",
    "# text_tokenized=[]\n",
    "# for sent in sentences:\n",
    "#     worter=nltk.word_tokenize(sentences[11],language='german')\n",
    "#     text_tokenized.append(worter)\n",
    "\n",
    "\n",
    "print(tokenized_text[90:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "18585409-71cf-4452-ad94-a12d187a9221"
    }
   },
   "source": [
    "# Lemmatisierung und Wortarterkennung (POS-Tagging)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leider enthält das NLTK Paket keine Lemmatisierer und Wortarterkenner (POS Tagger) für das Deutsche. \n",
    "\n",
    "Wir nutzen hier für beide Funtionen den Hanover Tagger (Siehe: [Christian Wartena (2019). A Probabilistic Morphology Model for German Lemmatization. In: Proceedings of the 15th Conference on Natural Language Processing (KONVENS 2019): Long Papers. Pp. 40-49, Erlangen.](https://doi.org/10.25968/opus-1527) )\n",
    "\n",
    "Wir müssen den Hanover Tagger zunächst (einmalig) herunterladen und installieren:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install HanTa # Hannover Tagger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir binden das Modul ein und laden ein vortrainiertes Modell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: HanTa in c:\\users\\mjzag\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.1.1)Note: you may need to restart the kernel to use updated packages.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pip install HanTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from HanTa import HanoverTagger as ht\n",
    "\n",
    "tagger = ht.HanoverTagger('morphmodel_ger.pgz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir können jetzt Wörter oder Sätze analysieren. Die Funktion _analyze()_ gibt ein Lemma und Wortart für eine Wortform:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Fachmarkt', 'NN')\n"
     ]
    }
   ],
   "source": [
    "print(tagger.analyze('Fachmärkte'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mit der Funktion _tag sent()_ werden alle Wörter in einem Satz lemmatisiert und und getagt. Bei Mehrdeutigkeiten, wir die Wortart gewählt, die im Kontext am wahrscheinlichsten ist. Wir versuchen das mal mit unserem Beispielsatz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Assyrer', 'Assyrer', 'NN'), ('im', 'in', 'APPRART'), ('engeren', 'eng', 'ADJ(A)'), ('Sinn', 'Sinn', 'NN'), ('gehören', 'gehören', 'VV(FIN)'), ('zu', 'zu', 'APPR'), ('den', 'der', 'ART'), ('nestorianischen', 'nestorianisch', 'ADJ(A)'), ('Christen', 'Christ', 'NN'), ('.', '.', '$.')]\n"
     ]
    }
   ],
   "source": [
    "tags = tagger.tag_sent(tokenized_sent)\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assyrer\n",
      "Sinn\n",
      "Christen\n"
     ]
    }
   ],
   "source": [
    "for t in tags:\n",
    "    if t[2]=='NN' or t[2]=='NE':\n",
    "        print(t[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weitere Möglichkeiten des Hanover Taggers werden [hier](https://github.com/wartaal/HanTa/blob/master/Demo.ipynb) beschrieben."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbpresent": {
   "slides": {},
   "themes": {}
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "159px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
